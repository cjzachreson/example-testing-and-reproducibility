# Cameron Zachreson
### date: April 30th, 2023

I will attempt to implement the exercise as published on April 28th, 2023

## some potential issues I aim to address: 
lack of specificity in exercise prompts
lack of generality in testing protocol (i.e., specific to python) 


# Exercise 1: 

current prompt: Define a reproducible environment in which the model can run

## CZ implementation: 

### OS and IDE

 - Operating System: Windows 10 Version 21H2 (OS Build 19044.2846)
 - IDE: Spyder version 4.2.5

### Packages

 - numpy 1.20.1

### new code: 
implemented some functions to output build info. The script 
```sh
build_info.py
```
contains functions for printing information about the operation system, the python version, and any list of modules that contain the 
```sh
.__version__
```
attribute. I've added some of these to 
```sh
test.py
```
so that the running OS, python, and numpy versions are printed to console. It would be good to print these into an output file generated by the test script. This can be part of Exercise 2. 


# Exercise 2: 

current prompt: Update the model so that its outputs can be reproduced

## CZ Implementation 

### reproducibility issues: 
 - the random number generator is set to the default, which is OK, but it would be good to print out any available details
 - the rng seed is not defined, which means each run will produce different output
 - regarding the above, the number of samples is small, so the model will sometimes pass the error tests, and sometimes not. 

### potential solutions: 
 - produce a configuration file that gives build info and some specific details about the model (i.e., what seed and rng are used) 
 - perform a test that compares a previously-generated sample set, to a new one that uses the same seed and sample size
 - note that there is no simple solution for the statistical tests. The tolerance and sample size are subjective, and there is no guarantee that these tests will be able to distinguish intrinsic variation (i.e., different results of the same model) from variation produced by model error (i.e., similar results from an incorrect impelementation of the same model or from an entirely different model). However, it makes sense to ensure that the tolerance is such that the same model run with different rng seeds will (almost always) pass the tests. 

